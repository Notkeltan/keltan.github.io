---
layout: post
title: "Algorithms and Emulators"
date: 2025-01-03
tags: [ai-alignment, algorithms, consciousness, prediction-systems, behavioral-analysis, neural-networks,social-media,tiktok,youtube,pinterest]
---

*The music I listened to while writing this. Good ambience - IMO - is essential to enjoying reading*

![](https://www.youtube.com/watch?v=Hc8gknCZ8uc&list=PLCFn0TPUM1fuOoNNjvw9ZPquMez02cfmv&index=4)

---
This is a ramble of thoughts I had. I recommend opening a .txt and spinning your own thoughts off with mine. You'll probably have some interesting insights because this is so branching and fractal. 

Here are Keywords generate by Claude 3.6 Sonnet.
Keywords: AI Alignment, AIXI, Afferent Nerves, Algorithm Ethics, Algorithm Optimization, Behavioral Analysis, Behavioral Feedback Loops, Behavioral Modeling, Behavioral Prediction, Brain Computing, Brain-Computer Interfaces, Business Models, Consciousness, Data Collection, Digital Interfaces, Digital Psychology, Efferent Nerves, Embedded Agency, Engagement Metrics, Human Behavior, Long-term Usage, Machine Consciousness, Machine Learning, Motor Output, Nerve Signals, Neural Interface, Neural Networks, Neural Processing, Neuroscience, Observer Systems, Oracle Systems, Pinterest, Prediction Engines, Prediction Systems, Predictive Systems, Sensory Input, Social Media, System Design, TikTok, User Behavior, User Engagement, User Experience

---


A human brain may contain many more FLOPS than the best data centres we currently train AI on. This is based on my current knowledge. It's probably wrong.

However, Imagine an AI trained on a human brain. Would that AI be more likely aligned? If it were designed to exactly mimic the human mind and trained on it?

How could you do that?

To train an AI I think you need a [[what is a system|System]] (something with an input and an output)

The hardware input of the brain are afferent nerves. These mostly connect to the basil ganglia. Though many come in all over the body - if we think of the body as a whole system here - so perhaps we need a whole body view.

We could take this view.

Then, we could have the AI as an observer and predictor. It observes input, and tries to predict the output the human will make.

If we were to do this with an outside view. Much like my [[Data Reanimation]] idea. Then what do we get? Does the AI actually correctly optimise?

Yes, but not perfectly I think. Unless maybe it has a lot of compute.

Can we relate this to [[AIXI]] in a way that it is an outside observer of a system?

Or, as I have mostly done in the past. We can think of this as a recommendation algorithm like the [[YouTube Algorithm]] or [[TikTok Algorithm]].

These (especially TikTok) are damn good algorithms for finding what a Sapien wants to watch. It isn't perfectly optimised for this however, It is optimised to keep engagement. And also to do so over long horizons (I assume).

An app like TikTok that hooks you as soon as you open the app and doesn't let you go is actually bad for the app. (I have heard it takes 50 videos the fist time you open TikTok on average to become addicted). 

If it did that, and you WOW (World of Warcraft) levels never left, pissing yourself and starving to death. Then the app wouldn't stay around for very long.
- Apple takes down immediately
- Govs shut it down
- Devs possibly get arrested or sued

Instead, an app that keeps you on for an hour before degrading what it serves you is a bit better. You won't then think things like (It's time to delete TikTok) actually, the ideal place to have someone is in the (I should delete TikTok) camp. Then keeping them perpetually on that fence. That's good because it gets the max time, with no actual app deletion action taken.

However, some people are more agentic and realise b/c of certain memes they pick up elsewhere or just because genetics that they just need to delete the app. but those people are rare enough that the business model still works even if that 0.001% choose to delete the app.

So, it's likely then that the apps could be even better at predicting what people want. We could just live in Wall-e world. But we don't because capitalism protects us.

>[!idea] #videoidea
>How capitalism protects us from Wall-E world

Of course this all relates to [[super stimuli]].

So back to our main topic. We can see that an AI can predict a human's actions very well. That is based on the input (videos shown) and the output (User Interaction) and those signals are optimised on a spectrum of:
$$
\boxed{\begin{matrix}
\text{User Engagement} & \xleftarrow{\hspace{0.5cm}} \xrightarrow{\hspace{5cm}} & \text{Long Term Usage} \\
\end{matrix}}
$$
If you weight too much on engagement (UE), you loose Long Term Usage. The example of that is WOW. If you weight too much on Long Term Usage (LTU), you loose out on long (UE) sessions, and the ability to show lots of ads. The example of this is harder to think of. But something like Google search a few years ago is good, where the business model was getting the user the information they came for ASAP and with little friction. These days Google is leaning further to the left side however, and we now have a worse search experience because of that.

### Pinterest Rant

A Hypothetical example is what Pinterest should be. Ideally Pinterest would minimise the amount of time you spent on site, and maximise the times you came back to use it again. That would look like
1. I need inspiration b/c I'm stuck on this creative problem
2. Open Pinterest
3. Scroll for a minute before I find exactly the thing that inspires me to create what I wanted to create
4. Leave Pinterest and keep building
5. More likely to come back in future

Unfortunately, Pinterest runs an Ad model. Thus, you get stuck on Pinterest at times when you least want to be sucked into a social media platform. And you end up with huge bloated boards on ideas that are almost good enough.

Ideally, Pinterest would offer a Pro Paid version. And give you a 'switch to Pro-Algorithm' option in the settings. However, they don't do this, and it is maximally annoying to me.

### Back to Main

The success of Buisnesses like TikTok shows that we can acuratly predict Sapien behaviour to a high degree with only extermal outputs and inputs. So much so that you end up with videos on your For You Page that you never thought you'd be interested in.

That's a feedback loop actually. I'm thinking of a particularly nerdy friend of mine, who at one point was obsessed with collecting video games and making movies.

Who is now stuck learning Football trivia I presume because TikTok found a small interest in Football, which it used to spin up a full fledged obsession in this person. Other platforms caught wind of this obsession, perhaps based on Google search and location data. And now he is algorithmically bombarded with Football knowledge. And I now have a best friend obsessed with something that I actively dislike b/c of some quirk with my [[genes]] or upbringing.

To be clear, I am not against Football or any sport (Except for the immoral such as Horse Racing) or the people who participate in these sports. I just hate sport. I can't give you a reason beyond it Aestetically discusts me in a weird way. The closest thing I can get into is esports or rocket launches. And even those are something I aspire to be interested in and just can't totally compel myself to have an interest in.

---
But what if we had something that wasn't predicting for the sake of profit, and instead predicting to be as good at being you as possible?

What pitfalls would it fall into?

Well, what type of system are we talking here? Is this system [[Embedded Agency (full-text version)|embedded]] into the same world as you? Is it able to change things about the inputs you get from the world?

If it can do this, You're fucked. In the same way as my [Football Friend](https://youtu.be/nNPUbLHk9oE?si=7xYzuMsEeuVqGvdV). Because now you've got something with the incentive to make you as predictable as possible. That's bad. 

### [[Alice's Restaurant]]  

So instead we need to make it external to you. Much more like [[AIXI]] or [[Embedded Agency (full-text version)#2025-01-02 09 00 22|Alexie]]. A thing that doesn't know it itself exists. Which can look at you as a system, and make predictions on you. It's more like an Oracle, an Oracle Observer, a prediction Engine; (Ob-E). 

Ob-E can look at you and have the following thoughts:
1. Alice is eating Pancakes with Bob
2. The input I predict Alice will get will be 'good'
3. I can make a note of this and check if Alice eats Pancakes again in the future
4. I should note though, if Alice only eats Pancakes with Bob in the future. Maybe Alice only eats Pancakes with Bob.
5. I predict that in the short term, Alice might say something like "Yummy! these are so good". Based on what she's said when eating food with ingredients and preparation that are similar to that of Pancakes.
6. *Alice Says "Yummy! These are so good"* Great! I'll increase the chances that Alice likes Pancakes based of this.
7. But I'll keep in mind that I haven't yet seen an example of her Eating Pancakes when not with Bob. It's still possible that Alice is saying she likes Pancakes just to make Bob happy.

Ob-E here is doing a good job of predicting Alice's behaviour. It has a correct model of Alice somewhere in itself. Or at least, a correct model of Alice eating Pancakes with Bob.

But Ob-E (I think) isn't Alice. I think that after a time you could replace the system that creates Alice with Ob-E and outside observers wouldn't be able to tell the difference. 

But Ob-E's underlying structure isn't Alice's underlying structure. Maybe there is a version of Alice inside of Ob-E that is conscious. Maybe not.

Here's something else. If Ob-E is only observing outside data. Like:
1. Alice input Pancake to Mouth
2. Alice output "Yummy!"

But what if there's this big gap in the middle of the input and the output which is Alice thinking "Wow! That's a lot of maple syrup. Oh, well. I'm with a friend so I can treat myself here."

Alice still input Pancake, Alice still output "Yummy!". But there is a critical part of what makes Alice Alice missing. The 'filling' of an Alice Sandwich 🥪! 

---
But is this really a big deal???

I don't know yet. Let me write about it to try and figure it out.

There are several things I can think of that would make Ob-E a better predictor. And one way we could maybe use $\text{Ob-E}_{Alice}$ to properly emulate Alice.

I'll start with the latter. Which includes a big of ✨Magic✨. That is:
1. Discover what consciousness is
2. Write a modular version of it (consiousness.py)
3. Slap Consiousness.py into $\text{Ob-E}_{Alice}$

Easy! But [[I Notice I'm Confused|idk]] what that would entail... So, lets go to other ways to make $\text{Ob-E}_{Alice}$ better.

Also, I'm changing $\text{Ob-E}_{Alice}$ to (Ob-Alice).

Here are two ways we could make Ob-Alice better:
1. On top of external data AKA (3rd person Data) we could have raw input and output data
2. We somehow hook Ob-E upto certain parts of Alice's brain and let it watch what's actually happening

### Nerve Signals

Afferent Nerve signals are input, Efferent Nerve signals are output.

Some people remember the difference because they understand the difference between Affect and Effect. I remember the difference because someone punching you makes you say "AAAAA!" 

The punch is the input, this only helps you remember Afferent Signals, but you can just remember that the other one is Efferent[^1].

*Questions*
1. If we hook Ob-E upto nerve signals, what happens?
2. What happens if we hook Ob-Alice up and train on these signals? 
3. What happens if we finetune Ob-Alice on these signals?

*Answers to all*
1. [[I Notice I'm Confused|idk]]


I look at things through the lens of Psychology. From my perspective, in the first question we Pavlovian Condition Ob-E to:
*Trigger:* 'Feel' an Afferent Nerve signal
*Action:* Predict Alice is about to do something

*Example:*
(In this example there are Pancakes in front of Alice)
1. Input from the Optic Nerve
2. Similar inputs in the past have indicated disk shaped cakes covered in syrup
3. Check with Ob-E external perspective confirms this
4. Predict that soon efferent signals will leave the [[brain stem]] and travel out of the [[Matter/Spinal cord|Spinal Cord]] [^2]
5. Afferent signals will soon return with signal indicating that Alice is moving her arm.
6. Afferent signals do return with this result
7. Ob-E external confirms this when it happens

This example actually includes a lot of ✨Magic✨ again. Like, How are we connecting Ob-E to the [[cranial nerves]] and [[spinal cord]] in a way that it's receiving all inputs from all Axons? Or, at least enough axons to predict correctly.

I should condense this model honestly. Something more like Ob-E receives a (1-0) when a nerve is active or not active. Or a Byte that says: 
- A = Afferent Signal
- B = No Afferent Signal
- C = Efferent Signal
- D = No Efferent Signal

The above example relies too heavily on granular signals being sent to Ob-E.

Like, we've kinda cracked digital eyes if Ob-E can be connected to the Optic Nerve in that way.

But there are other problems here too. Like, when is the optic nerve not sending Afferent signals? If Ob-E only gets (A) signal. What predictions can really be made based of this? For that reason, the model needs the extra complexity. 

And if Ob-E is only connected to the origin of these Nerves, it misses a bunch of critical data. Like, if Efferent signal is sent to trigeminal, what branch that signal travels along is super important for determining if we're seeing chewing action, or adjusting the size of the Iris.

But, even with these limitations, I think we still get an Ob-E that is closer to what Alice actually is. That's because it is closer to hugging the problem of Alice. It is closer to what Alice really is, a system of Nerve impulses.

This would allow for lower level prediction of Alice. Thus, more accurate predictions. 

In an analogous manner, I bet TikTok could be even better at predicting you if it has these connections. If it knew the very moment your brain registers the video input, and the very moment it sends a signal to swipe to the next video.

Perhaps you're uninterested in a very short video, and it's able to play in it's entirety before you process that you don't like it and decide to swipe. (I think that TikTok probably already knows this though.)

